# End-to-End Agentic Research Workflow Architecture Specification

**Version:** 1.1  
**Author:** Gia Tenica*  
**Date:** 2 January 2026

*Gia Tenica is an anagram for Agentic AI. Gia is a fully autonomous AI researcher, for more information see: https://giatenica.com

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Phase 1: Foundation & Topology](#phase-1-foundation--topology)
   - [1.1 State Schema Definition](#11-state-schema-definition)
   - [1.2 High-Level Node Topology](#12-high-level-node-topology)
   - [1.3 Node Summary Table](#13-node-summary-table)
   - [1.4 State Flow Diagram](#14-state-flow-diagram)
3. [Phase 2: Node Deep Dive — Planner & Searcher](#phase-2-node-deep-dive--planner--searcher)
   - [2.1 PLANNER Node](#21-planner-node)
   - [2.2 SEARCHER Node](#22-searcher-node)
4. [Phase 3: Node Deep Dive — Analyst, Writer, Reviewer](#phase-3-node-deep-dive--analyst-writer-reviewer)
   - [3.1 ANALYST Node](#31-analyst-node)
   - [3.2 WRITER Node](#32-writer-node)
   - [3.3 REVIEWER Node](#33-reviewer-node)
5. [Phase 4: Orchestration, Persistence, & Safety](#phase-4-orchestration-persistence--safety)
   - [4.1 Edge Logic & Conditional Routing](#41-edge-logic--conditional-routing)
   - [4.2 Human-in-the-Loop (HITL) Breakpoints](#42-human-in-the-loop-hitl-breakpoints)
   - [4.3 Persistence & Thread State](#43-persistence--thread-state)
   - [4.4 Error Handling & Recovery](#44-error-handling--recovery)
   - [4.5 Complete Graph Visualization](#45-complete-graph-visualization)
6. [Summary](#summary)
7. [Appendix A: Requirements Traceability](#appendix-a-requirements-traceability)
8. [Appendix B: Configuration Reference](#appendix-b-configuration-reference)

---

## Executive Summary

This document specifies the architecture for a LangGraph-based Multi-Agent System (MAS) designed to execute complex research workflows. The system decomposes research queries, performs multi-step web searches, synthesizes findings, and implements self-critique/revision loops to ensure high-quality outputs.

**Key Capabilities:**
- **Query Decomposition**: Breaks complex research questions into atomic sub-questions
- **Multi-Source Search**: Executes parallel searches across web, academic, and news sources
- **Synthesis & Analysis**: Identifies themes, contradictions, and knowledge gaps
- **Self-Critique Loop**: Iterative revision based on structured quality criteria
- **Human Oversight**: Configurable HITL breakpoints for critical decisions
- **Graceful Degradation**: Fallback strategies when optimal output is not achievable

---

## Phase 1: Foundation & Topology

### 1.1 State Schema Definition

The workflow state is the central data structure that flows through all nodes. We use Pydantic for validation and type safety.

```python
"""
State Schema for LangGraph Research Workflow
=============================================
Central state object that persists across all graph nodes.
"""

from __future__ import annotations

from datetime import datetime
from enum import Enum
from typing import Annotated, Any, Dict, List, Literal, Optional, Sequence
from uuid import uuid4

from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from pydantic import BaseModel, Field


class ResearchStatus(str, Enum):
    """Enumeration of workflow states."""
    PENDING = "pending"
    PLANNING = "planning"
    SEARCHING = "searching"
    ANALYZING = "analyzing"
    WRITING = "writing"
    REVIEWING = "reviewing"
    REVISING = "revising"
    COMPLETED = "completed"
    FAILED = "failed"


class SearchQuery(BaseModel):
    """Individual search query with metadata."""
    query_id: str = Field(default_factory=lambda: str(uuid4())[:8])
    query_text: str
    search_type: Literal["web", "academic", "news", "code"] = "web"
    priority: int = Field(default=1, ge=1, le=5)
    status: Literal["pending", "completed", "failed"] = "pending"
    created_at: datetime = Field(default_factory=datetime.utcnow)


class SearchResult(BaseModel):
    """Structured search result from any source."""
    result_id: str = Field(default_factory=lambda: str(uuid4())[:8])
    query_id: str
    source_url: str
    title: str
    snippet: str
    full_content: Optional[str] = None
    relevance_score: float = Field(default=0.0, ge=0.0, le=1.0)
    retrieved_at: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ResearchPlan(BaseModel):
    """Structured research plan generated by Planner node."""
    plan_id: str = Field(default_factory=lambda: str(uuid4())[:8])
    original_query: str
    decomposed_questions: List[str] = Field(default_factory=list)
    search_queries: List[SearchQuery] = Field(default_factory=list)
    methodology: str = ""
    expected_sections: List[str] = Field(default_factory=list)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    revised_at: Optional[datetime] = None


class AnalysisResult(BaseModel):
    """Output from the Analyst node."""
    analysis_id: str = Field(default_factory=lambda: str(uuid4())[:8])
    key_findings: List[str] = Field(default_factory=list)
    themes: List[str] = Field(default_factory=list)
    contradictions: List[str] = Field(default_factory=list)
    gaps: List[str] = Field(default_factory=list)
    confidence_score: float = Field(default=0.0, ge=0.0, le=1.0)
    source_citations: Dict[str, List[str]] = Field(default_factory=dict)


class DraftSection(BaseModel):
    """Individual section of the research output."""
    section_id: str
    title: str
    content: str
    citations: List[str] = Field(default_factory=list)
    word_count: int = 0


class ResearchDraft(BaseModel):
    """Complete research draft from Writer node."""
    draft_id: str = Field(default_factory=lambda: str(uuid4())[:8])
    title: str = ""
    abstract: str = ""
    sections: List[DraftSection] = Field(default_factory=list)
    conclusion: str = ""
    references: List[str] = Field(default_factory=list)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    version: int = 1


class CritiqueItem(BaseModel):
    """Individual critique point from Reviewer."""
    critique_id: str = Field(default_factory=lambda: str(uuid4())[:8])
    category: Literal[
        "accuracy", "completeness", "coherence", 
        "citation", "methodology", "style"
    ]
    severity: Literal["critical", "major", "minor", "suggestion"]
    location: str  # Section or general
    description: str
    suggested_fix: Optional[str] = None


class Critique(BaseModel):
    """Complete critique from Reviewer node."""
    critique_id: str = Field(default_factory=lambda: str(uuid4())[:8])
    items: List[CritiqueItem] = Field(default_factory=list)
    overall_score: float = Field(default=0.0, ge=0.0, le=10.0)
    pass_threshold: float = 7.5
    recommendation: Literal["approve", "revise", "reject"] = "revise"
    summary: str = ""
    created_at: datetime = Field(default_factory=datetime.utcnow)

    @property
    def passes_review(self) -> bool:
        """Check if critique score meets threshold."""
        return self.overall_score >= self.pass_threshold and self.recommendation == "approve"

    @property
    def critical_count(self) -> int:
        """Count critical issues."""
        return sum(1 for item in self.items if item.severity == "critical")


class WorkflowState(BaseModel):
    """
    Central State Schema for LangGraph Research Workflow
    =====================================================
    
    This state object flows through all nodes and maintains
    the complete context of the research workflow.
    
    Uses Pydantic for validation with LangGraph message annotation
    for proper message accumulation.
    """
    
    # === Core Identifiers ===
    workflow_id: str = Field(default_factory=lambda: str(uuid4()))
    status: ResearchStatus = ResearchStatus.PENDING
    
    # === Message History ===
    # Using LangGraph's add_messages reducer for proper accumulation
    messages: Annotated[Sequence[AnyMessage], add_messages] = Field(default_factory=list)
    
    # === Research Planning ===
    original_query: str = ""
    research_plan: Optional[ResearchPlan] = None
    
    # === Data Collection ===
    search_results: List[SearchResult] = Field(default_factory=list)
    collected_data: Dict[str, Any] = Field(default_factory=dict)
    
    # === Analysis & Synthesis ===
    analysis: Optional[AnalysisResult] = None
    draft: Optional[ResearchDraft] = None
    
    # === Review & Iteration ===
    critique: Optional[Critique] = None
    iteration_count: int = 0
    max_iterations: int = 3
    
    # === Workflow Metadata ===
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    errors: List[str] = Field(default_factory=list)
    checkpoints: List[str] = Field(default_factory=list)
    
    # === Configuration ===
    config: Dict[str, Any] = Field(default_factory=dict)

    class Config:
        """Pydantic configuration."""
        arbitrary_types_allowed = True

    def add_checkpoint(self, checkpoint: str) -> None:
        """Record a workflow checkpoint."""
        self.checkpoints.append(f"{datetime.utcnow().isoformat()}: {checkpoint}")
        self.updated_at = datetime.utcnow()

    def add_error(self, error: str) -> None:
        """Record an error."""
        self.errors.append(f"{datetime.utcnow().isoformat()}: {error}")

    @property
    def can_iterate(self) -> bool:
        """Check if more iterations are allowed."""
        return self.iteration_count < self.max_iterations

    @property
    def pending_searches(self) -> List[SearchQuery]:
        """Get searches that haven't been executed."""
        if not self.research_plan:
            return []
        return [q for q in self.research_plan.search_queries if q.status == "pending"]
```

### 1.2 High-Level Node Topology

The workflow follows a directed acyclic graph (DAG) pattern with a critique-revision loop:

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#4a90d9', 'primaryTextColor': '#fff', 'primaryBorderColor': '#2c5aa0', 'lineColor': '#5c6bc0', 'secondaryColor': '#81c784', 'tertiaryColor': '#ffb74d'}}}%%

flowchart TD
    subgraph ENTRY["Entry Point"]
        START([START]) --> INPUT[/"Research Query"/]
    end

    subgraph PLANNING["Planning Phase"]
        INPUT --> PLANNER["PLANNER<br/>─────────<br/>• Decompose query<br/>• Generate sub-questions<br/>• Create search plan"]
    end

    subgraph COLLECTION["Data Collection Phase"]
        PLANNER --> SEARCHER["SEARCHER<br/>─────────<br/>• Execute searches<br/>• Parse results<br/>• Validate sources"]
        SEARCHER --> CHECK_SEARCHES{{"All searches<br/>complete?"}}
        CHECK_SEARCHES -->|No| SEARCHER
        CHECK_SEARCHES -->|Yes| ANALYST
    end

    subgraph SYNTHESIS["Synthesis Phase"]
        ANALYST["ANALYST<br/>─────────<br/>• Synthesize findings<br/>• Identify themes<br/>• Detect gaps"]
        ANALYST --> NEED_MORE{{"Data gaps<br/>found?"}}
        NEED_MORE -->|Yes, minor| SEARCHER
        NEED_MORE -->|No| WRITER
    end

    subgraph DRAFTING["Drafting Phase"]
        WRITER["WRITER<br/>─────────<br/>• Structure content<br/>• Generate sections<br/>• Add citations"]
    end

    subgraph REVIEW["Review Loop"]
        WRITER --> REVIEWER["REVIEWER<br/>─────────<br/>• Critique draft<br/>• Score quality<br/>• Identify issues"]
        REVIEWER --> DECISION{{"Quality<br/>threshold<br/>met?"}}
        DECISION -->|"No & iterations < max"| REVISE["REVISE<br/>─────────<br/>• Apply feedback<br/>• Update sections"]
        REVISE --> WRITER
        DECISION -->|"No & iterations >= max"| FALLBACK["FALLBACK<br/>─────────<br/>• Return best effort<br/>• Log limitations"]
    end

    subgraph EXIT["Exit Point"]
        DECISION -->|Yes| OUTPUT[/"Final Research Output"/]
        FALLBACK --> OUTPUT
        OUTPUT --> END([END])
    end

    subgraph HITL["Human-in-the-Loop Breakpoints"]
        direction LR
        BP1["After Planning"]
        BP2["Before Final Output"]
    end

    PLANNER -.->|"Optional HITL"| BP1
    BP1 -.-> SEARCHER
    REVIEWER -.->|"Optional HITL"| BP2
    BP2 -.-> OUTPUT

    classDef entryExit fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    classDef planning fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    classDef collection fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef synthesis fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef drafting fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef review fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef decision fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    classDef hitl fill:#eceff1,stroke:#546e7a,stroke-width:1px,stroke-dasharray: 5 5

    class START,END entryExit
    class INPUT,OUTPUT entryExit
    class PLANNER planning
    class SEARCHER,CHECK_SEARCHES collection
    class ANALYST,NEED_MORE synthesis
    class WRITER drafting
    class REVIEWER,DECISION,REVISE,FALLBACK review
    class BP1,BP2 hitl
```

### 1.3 Node Summary Table

| Node | Purpose | Input | Output | Tools |
|------|---------|-------|--------|-------|
| **PLANNER** | Decompose research query into actionable plan | `original_query` | `research_plan` | None (LLM only) |
| **SEARCHER** | Execute searches and collect data | `search_queries` | `search_results` | Web API, Academic API |
| **ANALYST** | Synthesize and analyze collected data | `search_results` | `analysis` | None (LLM only) |
| **WRITER** | Generate structured research output | `analysis`, `research_plan` | `draft` | None (LLM only) |
| **REVIEWER** | Critique and score the draft | `draft` | `critique` | None (LLM only) |

### 1.4 State Flow Diagram

```
┌─────────────────────────────────────────────────────────────────────┐
│                        WORKFLOW STATE FLOW                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  START ──► [original_query] ──► PLANNER                            │
│                                    │                                │
│                                    ▼                                │
│                            [research_plan]                          │
│                                    │                                │
│                                    ▼                                │
│  ┌─────────────────────────► SEARCHER ◄──────────────┐             │
│  │                              │                     │             │
│  │                              ▼                     │             │
│  │                      [search_results]              │             │
│  │                              │                     │             │
│  │                              ▼                     │             │
│  │                          ANALYST                   │             │
│  │                              │                     │             │
│  │ (gap found)                  ▼                     │             │
│  └──────────────────────── [analysis] ───────────────┘             │
│                                 │                                   │
│                                 ▼                                   │
│                              WRITER                                 │
│                                 │                                   │
│                                 ▼                                   │
│                             [draft]                                 │
│                                 │                                   │
│  ┌──────────────────────────────┼──────────────────────────────┐   │
│  │                              ▼                               │   │
│  │                          REVIEWER                            │   │
│  │                              │                               │   │
│  │                              ▼                               │   │
│  │                         [critique]                           │   │
│  │                              │                               │   │
│  │              ┌───────────────┼───────────────┐               │   │
│  │              │               │               │               │   │
│  │              ▼               ▼               ▼               │   │
│  │          [APPROVE]      [REVISE]        [REJECT]             │   │
│  │              │               │               │               │   │
│  │              │               │               │               │   │
│  │              ▼               ▼               ▼               │   │
│  │           OUTPUT ◄─── iteration++ ───► FALLBACK              │   │
│  │                          (loop)              │               │   │
│  │                              │               │               │   │
│  │                              ▼               ▼               │   │
│  │                      [iteration_count]   OUTPUT              │   │
│  └──────────────────────────────────────────────────────────────┘   │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

---

**Phase 1 Complete.** ✓

---

## Phase 2: Node Deep Dive — Planner & Searcher

### 2.1 PLANNER Node

The Planner is the strategic brain of the workflow. It decomposes complex research queries into structured, actionable plans using Chain-of-Thought reasoning.

#### 2.1.1 Input/Output Schema

```python
"""
Planner Node - Input/Output Specifications
==========================================
"""

from pydantic import BaseModel, Field
from typing import List, Optional
from langchain_core.messages import HumanMessage, AIMessage


class PlannerInput(BaseModel):
    """Input schema for Planner node."""
    
    original_query: str = Field(
        ...,
        description="The user's research question or topic",
        min_length=10,
        max_length=2000
    )
    
    context: Optional[str] = Field(
        default=None,
        description="Additional context or constraints for the research"
    )
    
    domain_hints: List[str] = Field(
        default_factory=list,
        description="Suggested domains to focus on (e.g., 'academic', 'industry')"
    )
    
    depth: str = Field(
        default="standard",
        description="Research depth: 'quick', 'standard', 'comprehensive'"
    )


class PlannerOutput(BaseModel):
    """Output schema for Planner node."""
    
    research_plan: ResearchPlan  # From Phase 1 schema
    
    reasoning_trace: str = Field(
        ...,
        description="Chain-of-thought reasoning for plan decisions"
    )
    
    confidence: float = Field(
        default=0.8,
        ge=0.0,
        le=1.0,
        description="Planner's confidence in the plan quality"
    )
    
    warnings: List[str] = Field(
        default_factory=list,
        description="Any concerns about query ambiguity or scope"
    )
```

#### 2.1.2 Internal Logic — Chain-of-Thought Strategy

```python
"""
Planner Node Implementation
===========================
Uses Chain-of-Thought (CoT) prompting for systematic query decomposition.
"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langgraph.graph import StateGraph

# === PLANNER SYSTEM PROMPT ===

PLANNER_SYSTEM_PROMPT = """You are a Senior Research Strategist specializing in decomposing complex research queries into actionable investigation plans.

## Your Role
Transform ambiguous or complex research questions into structured, executable research plans with clear sub-questions and search strategies.

## Core Principles
1. **Decomposition First**: Break complex queries into atomic, answerable sub-questions
2. **Coverage Mapping**: Ensure sub-questions collectively cover the original query
3. **Search Diversity**: Design queries for multiple source types (academic, web, news)
4. **Feasibility Check**: Flag queries that may be unanswerable or require clarification

## Output Requirements
- Generate 3-7 decomposed sub-questions
- Create 5-15 specific search queries with source type annotations
- Define expected output sections that map to sub-questions
- Provide methodology notes for the research approach

Today's date: {current_date}
"""

PLANNER_COT_TEMPLATE = """## Research Query
{original_query}

## Additional Context
{context}

## Domain Focus
{domain_hints}

## Requested Depth
{depth}

---

## Your Task
Analyze this research query using the following Chain-of-Thought process:

### Step 1: Query Understanding
- What is the core question being asked?
- What are the implicit assumptions?
- What scope boundaries exist?

### Step 2: Decomposition
- What are the essential sub-questions that must be answered?
- Are there dependent questions (must answer A before B)?
- What are the independent parallel questions?

### Step 3: Search Strategy
- What types of sources are most relevant? (academic papers, news, industry reports, etc.)
- What specific search queries will yield the needed information?
- What are fallback queries if primary searches fail?

### Step 4: Output Planning
- What sections should the final output contain?
- How do sections map to sub-questions?
- What is the logical flow of the research narrative?

### Step 5: Risk Assessment
- What aspects might be difficult to research?
- Are there potential biases in available sources?
- What clarifications might be needed from the user?

---

Now provide your structured research plan:

{format_instructions}
"""


async def planner_node(state: WorkflowState) -> dict:
    """
    PLANNER Node Implementation
    
    Executes Chain-of-Thought decomposition of the research query
    and generates a structured research plan.
    
    Args:
        state: Current workflow state with original_query
        
    Returns:
        Updated state dict with research_plan populated
    """
    from datetime import datetime
    from langchain_openai import ChatOpenAI
    
    # Initialize LLM (configurable model)
    llm = ChatOpenAI(
        model=state.config.get("planner_model", "gpt-4o"),
        temperature=0.3,  # Lower temp for structured planning
        max_tokens=4000
    )
    
    # Setup output parser
    parser = PydanticOutputParser(pydantic_object=ResearchPlan)
    
    # Build prompt
    prompt = ChatPromptTemplate.from_messages([
        ("system", PLANNER_SYSTEM_PROMPT),
        ("human", PLANNER_COT_TEMPLATE)
    ])
    
    # Format inputs
    chain_input = {
        "current_date": datetime.now().strftime("%Y-%m-%d"),
        "original_query": state.original_query,
        "context": state.config.get("context", "None provided"),
        "domain_hints": ", ".join(state.config.get("domain_hints", ["general"])),
        "depth": state.config.get("depth", "standard"),
        "format_instructions": parser.get_format_instructions()
    }
    
    # Execute chain
    chain = prompt | llm | parser
    
    try:
        research_plan = await chain.ainvoke(chain_input)
        
        # Enrich plan with metadata
        research_plan.original_query = state.original_query
        
        # Generate search queries from decomposed questions
        if not research_plan.search_queries:
            research_plan.search_queries = _generate_search_queries(
                research_plan.decomposed_questions,
                state.config.get("depth", "standard")
            )
        
        return {
            "research_plan": research_plan,
            "status": ResearchStatus.PLANNING,
            "messages": [AIMessage(content=f"Research plan created with {len(research_plan.decomposed_questions)} sub-questions and {len(research_plan.search_queries)} search queries.")]
        }
        
    except Exception as e:
        return {
            "errors": [f"Planner failed: {str(e)}"],
            "status": ResearchStatus.FAILED
        }


def _generate_search_queries(
    questions: List[str], 
    depth: str
) -> List[SearchQuery]:
    """
    Generate search queries from decomposed questions.
    
    Maps depth to query count:
    - quick: 1 query per question
    - standard: 2 queries per question (web + academic)
    - comprehensive: 3+ queries per question
    """
    queries = []
    
    search_types = {
        "quick": ["web"],
        "standard": ["web", "academic"],
        "comprehensive": ["web", "academic", "news"]
    }
    
    types_to_use = search_types.get(depth, ["web", "academic"])
    
    for i, question in enumerate(questions):
        for search_type in types_to_use:
            queries.append(SearchQuery(
                query_text=question,
                search_type=search_type,
                priority=min(i + 1, 5)  # Earlier questions = higher priority
            ))
    
    return queries


def validate_research_plan(plan: ResearchPlan) -> tuple[bool, List[str]]:
    """
    Validate a research plan before execution.
    
    Returns:
        Tuple of (is_valid, list_of_issues)
    """
    issues = []
    
    if not plan.decomposed_questions:
        issues.append("No decomposed questions generated")
    
    if len(plan.decomposed_questions) < 2:
        issues.append("Insufficient question decomposition (minimum 2)")
    
    if not plan.search_queries:
        issues.append("No search queries generated")
    
    if not plan.expected_sections:
        issues.append("No expected output sections defined")
    
    return len(issues) == 0, issues
```

---

### 2.2 SEARCHER Node

The Searcher executes the research plan by performing multi-source searches and aggregating results. It uses a ReAct pattern for adaptive search refinement.

#### 2.2.1 Input/Output Schema

```python
"""
Searcher Node - Input/Output Specifications
===========================================
"""

class SearcherInput(BaseModel):
    """Input schema for Searcher node."""
    
    search_queries: List[SearchQuery] = Field(
        ...,
        description="List of search queries to execute"
    )
    
    max_results_per_query: int = Field(
        default=10,
        ge=1,
        le=50,
        description="Maximum results to retrieve per query"
    )
    
    timeout_seconds: int = Field(
        default=30,
        description="Timeout for each search operation"
    )


class SearcherOutput(BaseModel):
    """Output schema for Searcher node."""
    
    search_results: List[SearchResult] = Field(
        default_factory=list,
        description="Collected search results"
    )
    
    queries_completed: int = Field(
        default=0,
        description="Number of queries successfully executed"
    )
    
    queries_failed: int = Field(
        default=0,
        description="Number of queries that failed"
    )
    
    coverage_report: dict = Field(
        default_factory=dict,
        description="Mapping of questions to result counts"
    )
```

#### 2.2.2 Tool Definitions — Search APIs

```python
"""
Search Tool Definitions
=======================
LangChain-compatible tools for multi-source search.
"""

from langchain_core.tools import tool, StructuredTool
from langchain_community.utilities import GoogleSerperAPIWrapper
from typing import Literal
import httpx
import asyncio


# === WEB SEARCH TOOL ===

@tool
async def web_search(
    query: str,
    num_results: int = 10
) -> List[dict]:
    """
    Execute a web search using Serper API.
    
    Args:
        query: Search query string
        num_results: Number of results to return
        
    Returns:
        List of search results with title, url, snippet
    """
    serper = GoogleSerperAPIWrapper(k=num_results)
    
    try:
        raw_results = await asyncio.to_thread(serper.results, query)
        
        results = []
        for item in raw_results.get("organic", [])[:num_results]:
            results.append({
                "title": item.get("title", ""),
                "url": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "source_type": "web"
            })
        
        return results
        
    except Exception as e:
        return [{"error": str(e), "query": query}]


# === ACADEMIC SEARCH TOOL ===

@tool
async def academic_search(
    query: str,
    num_results: int = 10,
    year_from: int = None
) -> List[dict]:
    """
    Search academic papers using Semantic Scholar API.
    
    Args:
        query: Search query string
        num_results: Number of results to return
        year_from: Filter papers from this year onwards
        
    Returns:
        List of academic paper metadata
    """
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    
    params = {
        "query": query,
        "limit": min(num_results, 100),
        "fields": "title,authors,year,abstract,url,citationCount,venue"
    }
    
    if year_from:
        params["year"] = f"{year_from}-"
    
    async with httpx.AsyncClient(timeout=30) as client:
        try:
            response = await client.get(base_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            results = []
            for paper in data.get("data", []):
                authors = ", ".join([
                    a.get("name", "") for a in paper.get("authors", [])[:3]
                ])
                if len(paper.get("authors", [])) > 3:
                    authors += " et al."
                
                results.append({
                    "title": paper.get("title", ""),
                    "url": paper.get("url", ""),
                    "snippet": paper.get("abstract", "")[:500] if paper.get("abstract") else "",
                    "authors": authors,
                    "year": paper.get("year"),
                    "citations": paper.get("citationCount", 0),
                    "venue": paper.get("venue", ""),
                    "source_type": "academic"
                })
            
            return results
            
        except Exception as e:
            return [{"error": str(e), "query": query}]


# === NEWS SEARCH TOOL ===

@tool
async def news_search(
    query: str,
    num_results: int = 10,
    days_back: int = 30
) -> List[dict]:
    """
    Search recent news articles using NewsAPI.
    
    Args:
        query: Search query string
        num_results: Number of results to return
        days_back: How many days back to search
        
    Returns:
        List of news articles
    """
    import os
    from datetime import datetime, timedelta
    
    api_key = os.getenv("NEWS_API_KEY")
    if not api_key:
        return [{"error": "NEWS_API_KEY not configured"}]
    
    base_url = "https://newsapi.org/v2/everything"
    
    from_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
    
    params = {
        "q": query,
        "from": from_date,
        "sortBy": "relevancy",
        "pageSize": min(num_results, 100),
        "apiKey": api_key
    }
    
    async with httpx.AsyncClient(timeout=30) as client:
        try:
            response = await client.get(base_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            results = []
            for article in data.get("articles", []):
                results.append({
                    "title": article.get("title", ""),
                    "url": article.get("url", ""),
                    "snippet": article.get("description", ""),
                    "source": article.get("source", {}).get("name", ""),
                    "published_at": article.get("publishedAt", ""),
                    "source_type": "news"
                })
            
            return results
            
        except Exception as e:
            return [{"error": str(e), "query": query}]


# === CONTENT PARSER TOOL ===

@tool
async def parse_webpage(url: str) -> dict:
    """
    Extract and parse content from a webpage.
    
    Args:
        url: URL to fetch and parse
        
    Returns:
        Parsed content with title, text, and metadata
    """
    from bs4 import BeautifulSoup
    
    async with httpx.AsyncClient(timeout=30, follow_redirects=True) as client:
        try:
            response = await client.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Remove script and style elements
            for element in soup(["script", "style", "nav", "footer", "header"]):
                element.decompose()
            
            # Extract text
            text = soup.get_text(separator="\n", strip=True)
            
            # Truncate to reasonable length
            max_chars = 10000
            if len(text) > max_chars:
                text = text[:max_chars] + "... [truncated]"
            
            return {
                "url": url,
                "title": soup.title.string if soup.title else "",
                "content": text,
                "word_count": len(text.split()),
                "success": True
            }
            
        except Exception as e:
            return {
                "url": url,
                "error": str(e),
                "success": False
            }
```

#### 2.2.3 Internal Logic — ReAct Pattern

```python
"""
Searcher Node Implementation
============================
Uses ReAct (Reasoning + Acting) pattern for adaptive search execution.
"""

SEARCHER_REACT_PROMPT = """You are a Research Data Collector executing a search plan.

## Available Tools
- web_search(query, num_results): General web search
- academic_search(query, num_results, year_from): Academic paper search
- news_search(query, num_results, days_back): Recent news search
- parse_webpage(url): Extract full content from a URL

## Current Task
Execute the following search queries and collect relevant results.

## Search Queries to Execute
{search_queries}

## Instructions
For each query:
1. **Thought**: Consider which tool is most appropriate
2. **Action**: Execute the search tool
3. **Observation**: Review the results
4. **Thought**: Assess if results are sufficient or need refinement

If initial results are poor:
- Reformulate the query with synonyms or alternative phrasing
- Try a different search type (web vs academic)
- Narrow or broaden the scope as needed

## Quality Criteria
- Aim for 3-5 high-quality results per sub-question
- Prioritize recent sources (last 3 years for fast-moving topics)
- Prefer authoritative sources (academic, established news, official docs)
- Flag if a query returns no useful results

Begin execution:
"""


async def searcher_node(state: WorkflowState) -> dict:
    """
    SEARCHER Node Implementation
    
    Executes search queries using ReAct pattern for adaptive refinement.
    
    Args:
        state: Current workflow state with research_plan
        
    Returns:
        Updated state dict with search_results populated
    """
    from langchain_openai import ChatOpenAI
    from langgraph.prebuilt import create_react_agent
    
    if not state.research_plan:
        return {
            "errors": ["No research plan available for search execution"],
            "status": ResearchStatus.FAILED
        }
    
    # Get pending queries
    pending_queries = state.pending_searches
    
    if not pending_queries:
        return {
            "messages": [AIMessage(content="All searches already completed.")],
            "status": ResearchStatus.SEARCHING
        }
    
    # Initialize LLM for ReAct agent
    llm = ChatOpenAI(
        model=state.config.get("searcher_model", "gpt-4o-mini"),
        temperature=0.1  # Low temp for tool execution
    )
    
    # Define tools
    tools = [web_search, academic_search, news_search, parse_webpage]
    
    # Create ReAct agent
    react_agent = create_react_agent(llm, tools)
    
    # Format queries for prompt
    queries_text = "\n".join([
        f"{i+1}. [{q.search_type.upper()}] {q.query_text} (priority: {q.priority})"
        for i, q in enumerate(pending_queries)
    ])
    
    # Execute agent
    try:
        result = await react_agent.ainvoke({
            "messages": [HumanMessage(content=SEARCHER_REACT_PROMPT.format(
                search_queries=queries_text
            ))]
        })
        
        # Parse results and update state
        new_results = _extract_search_results(result, pending_queries)
        
        # Update query statuses
        for query in pending_queries:
            query.status = "completed"
        
        return {
            "search_results": state.search_results + new_results,
            "status": ResearchStatus.SEARCHING,
            "messages": [AIMessage(content=f"Collected {len(new_results)} new results from {len(pending_queries)} queries.")]
        }
        
    except Exception as e:
        # Mark queries as failed
        for query in pending_queries:
            query.status = "failed"
        
        return {
            "errors": [f"Search execution failed: {str(e)}"],
            "status": ResearchStatus.FAILED
        }


def _extract_search_results(
    agent_result: dict,
    queries: List[SearchQuery]
) -> List[SearchResult]:
    """
    Extract structured SearchResult objects from ReAct agent output.
    
    Parses tool call results and maps them to the appropriate queries.
    """
    results = []
    
    # Extract tool outputs from agent messages
    for message in agent_result.get("messages", []):
        if hasattr(message, "tool_calls"):
            for tool_call in message.tool_calls:
                tool_output = tool_call.get("output", [])
                
                if isinstance(tool_output, list):
                    for item in tool_output:
                        if isinstance(item, dict) and "url" in item:
                            # Find matching query
                            query_id = _match_result_to_query(item, queries)
                            
                            results.append(SearchResult(
                                query_id=query_id,
                                source_url=item.get("url", ""),
                                title=item.get("title", ""),
                                snippet=item.get("snippet", ""),
                                full_content=item.get("content"),
                                relevance_score=_compute_relevance(item),
                                metadata={
                                    "source_type": item.get("source_type", "web"),
                                    "authors": item.get("authors"),
                                    "year": item.get("year"),
                                    "citations": item.get("citations")
                                }
                            ))
    
    return results


def _match_result_to_query(
    result: dict, 
    queries: List[SearchQuery]
) -> str:
    """Match a search result to its originating query."""
    # Simple heuristic: match by source type
    source_type = result.get("source_type", "web")
    
    for query in queries:
        if query.search_type == source_type and query.status == "pending":
            return query.query_id
    
    # Fallback to first pending query
    for query in queries:
        if query.status == "pending":
            return query.query_id
    
    return queries[0].query_id if queries else "unknown"


def _compute_relevance(result: dict) -> float:
    """
    Compute a relevance score for a search result.
    
    Factors:
    - Has substantial snippet: +0.3
    - Has full content: +0.2
    - Is academic with citations: +0.2 per 100 citations (max 0.3)
    - Recent (has year >= 2023): +0.2
    """
    score = 0.0
    
    if result.get("snippet") and len(result["snippet"]) > 100:
        score += 0.3
    
    if result.get("content"):
        score += 0.2
    
    citations = result.get("citations", 0)
    if citations:
        score += min(0.3, citations / 100 * 0.1)
    
    year = result.get("year")
    if year and int(year) >= 2023:
        score += 0.2
    
    return min(1.0, score)
```

---

**Phase 2 Complete.** ✓

---

## Phase 3: Node Deep Dive — Analyst, Writer, Reviewer

### 3.1 ANALYST Node

The Analyst synthesizes raw search results into structured insights, identifies themes, detects contradictions, and flags knowledge gaps.

#### 3.1.1 Input/Output Schema

```python
"""
Analyst Node - Input/Output Specifications
==========================================
"""

class AnalystInput(BaseModel):
    """Input schema for Analyst node."""
    
    search_results: List[SearchResult] = Field(
        ...,
        description="Collected search results to analyze"
    )
    
    research_plan: ResearchPlan = Field(
        ...,
        description="Original research plan for context alignment"
    )
    
    previous_analysis: Optional[AnalysisResult] = Field(
        default=None,
        description="Prior analysis if this is a refinement pass"
    )


class AnalystOutput(BaseModel):
    """Output schema for Analyst node."""
    
    analysis: AnalysisResult  # From Phase 1 schema
    
    data_sufficiency: Literal["sufficient", "gaps_minor", "gaps_major"] = Field(
        default="sufficient",
        description="Assessment of data completeness"
    )
    
    recommended_searches: List[SearchQuery] = Field(
        default_factory=list,
        description="Additional searches if gaps detected"
    )
```

#### 3.1.2 Internal Logic — Synthesis Strategy

```python
"""
Analyst Node Implementation
===========================
Multi-pass synthesis with theme extraction and gap detection.
"""

ANALYST_SYSTEM_PROMPT = """You are a Senior Research Analyst specializing in synthesizing diverse information sources into coherent insights.

## Your Role
Transform raw search results into structured analysis that:
1. Extracts key findings aligned with research questions
2. Identifies recurring themes across sources
3. Detects contradictions or conflicting information
4. Flags gaps where evidence is insufficient

## Analysis Framework

### Evidence Grading
- **Strong**: Multiple independent sources agree, includes primary data
- **Moderate**: 2-3 sources agree, mostly secondary analysis
- **Weak**: Single source or conflicting information
- **Gap**: No relevant evidence found

### Theme Identification
Group findings into thematic clusters. Each theme should:
- Have at least 2 supporting sources
- Connect to a specific research sub-question
- Include confidence assessment

### Contradiction Handling
When sources disagree:
1. Document both positions clearly
2. Assess source credibility (academic > news > blog)
3. Note temporal factors (newer may supersede older)
4. Flag for human review if unresolvable

Today's date: {current_date}
"""

ANALYST_SYNTHESIS_TEMPLATE = """## Research Context
**Original Query**: {original_query}
**Sub-Questions**: 
{decomposed_questions}

## Collected Evidence
{search_results_formatted}

---

## Your Analysis Task

### Step 1: Evidence Mapping
For each sub-question, identify which search results provide relevant evidence.

### Step 2: Key Findings Extraction
Extract 5-10 key findings. Each finding should:
- Be a specific, factual statement
- Cite its source(s)
- Indicate evidence strength

### Step 3: Theme Synthesis
Identify 3-5 overarching themes that emerge from the evidence.

### Step 4: Contradiction Detection
List any conflicting information found across sources.

### Step 5: Gap Analysis
Identify sub-questions with insufficient evidence.
For each gap, suggest a refined search query.

### Step 6: Confidence Assessment
Rate overall confidence (0.0-1.0) based on:
- Evidence coverage
- Source quality
- Internal consistency

---

Provide your structured analysis:

{format_instructions}
"""


async def analyst_node(state: WorkflowState) -> dict:
    """
    ANALYST Node Implementation
    
    Synthesizes search results into structured analysis.
    
    Args:
        state: Current workflow state with search_results
        
    Returns:
        Updated state with analysis and gap assessment
    """
    from datetime import datetime
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import PydanticOutputParser
    
    if not state.search_results:
        return {
            "errors": ["No search results available for analysis"],
            "status": ResearchStatus.FAILED
        }
    
    llm = ChatOpenAI(
        model=state.config.get("analyst_model", "gpt-4o"),
        temperature=0.2
    )
    
    parser = PydanticOutputParser(pydantic_object=AnalysisResult)
    
    # Format search results for prompt
    results_formatted = _format_search_results(state.search_results)
    
    # Format decomposed questions
    questions_formatted = "\n".join([
        f"  {i+1}. {q}" 
        for i, q in enumerate(state.research_plan.decomposed_questions)
    ])
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", ANALYST_SYSTEM_PROMPT),
        ("human", ANALYST_SYNTHESIS_TEMPLATE)
    ])
    
    chain = prompt | llm | parser
    
    try:
        analysis = await chain.ainvoke({
            "current_date": datetime.now().strftime("%Y-%m-%d"),
            "original_query": state.original_query,
            "decomposed_questions": questions_formatted,
            "search_results_formatted": results_formatted,
            "format_instructions": parser.get_format_instructions()
        })
        
        # Assess data sufficiency
        gap_count = len(analysis.gaps)
        if gap_count == 0:
            sufficiency = "sufficient"
        elif gap_count <= 2:
            sufficiency = "gaps_minor"
        else:
            sufficiency = "gaps_major"
        
        # Generate additional searches if needed
        additional_searches = []
        if sufficiency != "sufficient":
            additional_searches = _generate_gap_searches(analysis.gaps)
        
        return {
            "analysis": analysis,
            "status": ResearchStatus.ANALYZING,
            "messages": [AIMessage(content=f"Analysis complete: {len(analysis.key_findings)} findings, {len(analysis.themes)} themes, {gap_count} gaps identified.")]
        }
        
    except Exception as e:
        return {
            "errors": [f"Analysis failed: {str(e)}"],
            "status": ResearchStatus.FAILED
        }


def _format_search_results(results: List[SearchResult]) -> str:
    """Format search results for LLM consumption."""
    formatted = []
    
    for i, result in enumerate(results, 1):
        entry = f"""
### Source {i}
- **Title**: {result.title}
- **URL**: {result.source_url}
- **Type**: {result.metadata.get('source_type', 'web')}
- **Relevance**: {result.relevance_score:.2f}

**Content**:
{result.snippet[:1000] if result.snippet else '[No snippet available]'}
"""
        formatted.append(entry)
    
    return "\n---\n".join(formatted)


def _generate_gap_searches(gaps: List[str]) -> List[SearchQuery]:
    """Generate targeted searches for identified gaps."""
    return [
        SearchQuery(
            query_text=gap,
            search_type="academic",  # Prefer academic for gaps
            priority=1  # High priority
        )
        for gap in gaps[:3]  # Limit to top 3 gaps
    ]
```

---

### 3.2 WRITER Node

The Writer transforms analysis into polished research output with proper structure, citations, and academic style.

#### 3.2.1 Input/Output Schema

```python
"""
Writer Node - Input/Output Specifications
=========================================
"""

class WriterInput(BaseModel):
    """Input schema for Writer node."""
    
    analysis: AnalysisResult = Field(
        ...,
        description="Synthesized analysis to write from"
    )
    
    research_plan: ResearchPlan = Field(
        ...,
        description="Plan containing expected sections"
    )
    
    previous_draft: Optional[ResearchDraft] = Field(
        default=None,
        description="Previous draft if revising"
    )
    
    critique: Optional[Critique] = Field(
        default=None,
        description="Critique to address in revision"
    )
    
    style_guide: str = Field(
        default="academic",
        description="Writing style: 'academic', 'technical', 'executive'"
    )


class WriterOutput(BaseModel):
    """Output schema for Writer node."""
    
    draft: ResearchDraft  # From Phase 1 schema
    
    revision_notes: List[str] = Field(
        default_factory=list,
        description="Notes on what was changed from previous draft"
    )
```

#### 3.2.2 Internal Logic — Structured Generation

```python
"""
Writer Node Implementation
==========================
Section-by-section generation with citation integration.
"""

WRITER_SYSTEM_PROMPT = """You are a Senior Research Writer producing high-quality research documents.

## Your Role
Transform structured analysis into polished research output that:
1. Follows academic/technical writing standards
2. Maintains logical flow between sections
3. Properly cites all claims
4. Uses precise, unambiguous language

## Writing Standards

### Citation Rules
- Every factual claim must have a citation
- Use inline citations: (Author, Year) or [Source Title]
- Group related citations: (Smith, 2023; Jones, 2024)
- Never fabricate citations

### Style Guidelines
- Active voice preferred
- Avoid hedging words (seems, appears, might)
- Define technical terms on first use
- Use parallel structure in lists

### BANNED WORDS (Never use these)
delve, realm, harness, unlock, tapestry, paradigm, cutting-edge, revolutionize,
landscape, crucial, pivotal, groundbreaking, leverage, synergy, innovative,
game-changer, holistic, transformative, seamless, robust, breakthrough, empower

### Section Structure
Each section should:
- Open with a clear topic sentence
- Present evidence in logical order
- Close with synthesis or transition

Today's date: {current_date}
"""

WRITER_SECTION_TEMPLATE = """## Writing Task

### Document Metadata
- **Title**: {title}
- **Style**: {style_guide}
- **Target Length**: {target_length} words

### Analysis Summary
**Key Findings**:
{key_findings}

**Themes**:
{themes}

### Expected Sections
{expected_sections}

### Available Citations
{citations}

{revision_instructions}

---

## Instructions

Generate a complete research document with:

1. **Title**: Concise, descriptive title
2. **Abstract**: 150-250 word summary
3. **Sections**: As specified in expected sections
4. **Conclusion**: Synthesis of main points
5. **References**: Formatted citation list

For each section:
- Ensure claims are supported by citations
- Maintain consistent terminology
- Create smooth transitions

{format_instructions}
"""

REVISION_INSTRUCTIONS_TEMPLATE = """
### REVISION MODE
You are revising a previous draft based on critique.

**Previous Draft Issues**:
{critique_items}

**Overall Feedback**:
{critique_summary}

**Required Changes**:
- Address all CRITICAL issues
- Address MAJOR issues where possible
- Consider MINOR suggestions

Focus your revision on the specific issues identified.
"""


async def writer_node(state: WorkflowState) -> dict:
    """
    WRITER Node Implementation
    
    Generates structured research document from analysis.
    
    Args:
        state: Current workflow state with analysis
        
    Returns:
        Updated state with draft
    """
    from datetime import datetime
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import PydanticOutputParser
    
    if not state.analysis:
        return {
            "errors": ["No analysis available for writing"],
            "status": ResearchStatus.FAILED
        }
    
    llm = ChatOpenAI(
        model=state.config.get("writer_model", "gpt-4o"),
        temperature=0.4,  # Slightly higher for creative writing
        max_tokens=8000
    )
    
    parser = PydanticOutputParser(pydantic_object=ResearchDraft)
    
    # Build revision instructions if applicable
    revision_instructions = ""
    if state.critique and state.draft:
        revision_instructions = REVISION_INSTRUCTIONS_TEMPLATE.format(
            critique_items=_format_critique_items(state.critique.items),
            critique_summary=state.critique.summary
        )
    
    # Format inputs
    key_findings = "\n".join([f"- {f}" for f in state.analysis.key_findings])
    themes = "\n".join([f"- {t}" for t in state.analysis.themes])
    expected_sections = "\n".join([
        f"{i+1}. {s}" for i, s in enumerate(state.research_plan.expected_sections)
    ])
    citations = _format_citations(state.analysis.source_citations)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", WRITER_SYSTEM_PROMPT),
        ("human", WRITER_SECTION_TEMPLATE)
    ])
    
    chain = prompt | llm | parser
    
    try:
        draft = await chain.ainvoke({
            "current_date": datetime.now().strftime("%Y-%m-%d"),
            "title": state.research_plan.original_query[:100],
            "style_guide": state.config.get("style_guide", "academic"),
            "target_length": state.config.get("target_length", 2000),
            "key_findings": key_findings,
            "themes": themes,
            "expected_sections": expected_sections,
            "citations": citations,
            "revision_instructions": revision_instructions,
            "format_instructions": parser.get_format_instructions()
        })
        
        # Update version if revising
        if state.draft:
            draft.version = state.draft.version + 1
        
        return {
            "draft": draft,
            "status": ResearchStatus.WRITING,
            "messages": [AIMessage(content=f"Draft v{draft.version} complete: {len(draft.sections)} sections, {sum(s.word_count for s in draft.sections)} words.")]
        }
        
    except Exception as e:
        return {
            "errors": [f"Writing failed: {str(e)}"],
            "status": ResearchStatus.FAILED
        }


def _format_critique_items(items: List[CritiqueItem]) -> str:
    """Format critique items for revision prompt."""
    formatted = []
    for item in items:
        formatted.append(
            f"- [{item.severity.upper()}] {item.category}: {item.description}"
            + (f"\n  Suggested fix: {item.suggested_fix}" if item.suggested_fix else "")
        )
    return "\n".join(formatted)


def _format_citations(source_citations: Dict[str, List[str]]) -> str:
    """Format available citations for writer."""
    formatted = []
    for source_id, citations in source_citations.items():
        formatted.append(f"[{source_id}]: {', '.join(citations)}")
    return "\n".join(formatted) if formatted else "No structured citations available."
```

---

### 3.3 REVIEWER Node

The Reviewer performs quality assessment using structured criteria and generates actionable feedback for the revision loop.

#### 3.3.1 Input/Output Schema

```python
"""
Reviewer Node - Input/Output Specifications
===========================================
"""

class ReviewerInput(BaseModel):
    """Input schema for Reviewer node."""
    
    draft: ResearchDraft = Field(
        ...,
        description="Draft to review"
    )
    
    analysis: AnalysisResult = Field(
        ...,
        description="Original analysis for fact-checking"
    )
    
    research_plan: ResearchPlan = Field(
        ...,
        description="Plan for completeness checking"
    )
    
    iteration_count: int = Field(
        default=0,
        description="Current revision iteration"
    )


class ReviewerOutput(BaseModel):
    """Output schema for Reviewer node."""
    
    critique: Critique  # From Phase 1 schema
    
    improvement_potential: float = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="Estimated improvement possible with revision"
    )
```

#### 3.3.2 Critique Loop Criteria

```python
"""
Review Criteria and Scoring
===========================
"""

REVIEW_CRITERIA = {
    "accuracy": {
        "weight": 0.25,
        "description": "Factual correctness and citation accuracy",
        "checks": [
            "Claims match cited sources",
            "No unsupported assertions",
            "Statistics are correctly reported",
            "No logical fallacies"
        ]
    },
    "completeness": {
        "weight": 0.20,
        "description": "Coverage of research questions",
        "checks": [
            "All sub-questions addressed",
            "Expected sections present",
            "Adequate depth per section",
            "No major gaps"
        ]
    },
    "coherence": {
        "weight": 0.20,
        "description": "Logical flow and structure",
        "checks": [
            "Clear thesis/argument",
            "Logical section ordering",
            "Smooth transitions",
            "Consistent terminology"
        ]
    },
    "citation": {
        "weight": 0.15,
        "description": "Proper attribution and references",
        "checks": [
            "All claims cited",
            "Citation format consistent",
            "References complete",
            "No citation fabrication"
        ]
    },
    "methodology": {
        "weight": 0.10,
        "description": "Research approach transparency",
        "checks": [
            "Methods clearly described",
            "Limitations acknowledged",
            "Scope appropriate",
            "Bias awareness"
        ]
    },
    "style": {
        "weight": 0.10,
        "description": "Writing quality and readability",
        "checks": [
            "Clear, concise language",
            "No banned words",
            "Appropriate tone",
            "Grammar and spelling"
        ]
    }
}

# Scoring thresholds
PASS_THRESHOLD = 7.5
REVISION_THRESHOLD = 5.0  # Below this, reject
MAX_CRITICAL_ISSUES = 0   # Any critical issue blocks approval
MAX_MAJOR_ISSUES = 3      # More than this blocks approval
```

#### 3.3.3 Internal Logic — Structured Critique

```python
"""
Reviewer Node Implementation
============================
Multi-criteria evaluation with structured feedback.
"""

REVIEWER_SYSTEM_PROMPT = """You are a Senior Research Reviewer performing rigorous quality assessment.

## Your Role
Evaluate research documents against strict criteria and provide actionable feedback.

## Review Criteria (Weighted Scoring)
{criteria_description}

## Severity Levels
- **CRITICAL**: Fundamental errors that invalidate findings (e.g., fabricated citations, major factual errors)
- **MAJOR**: Significant issues affecting quality (e.g., missing sections, logical gaps)
- **MINOR**: Polish issues that should be fixed (e.g., unclear phrasing, minor inconsistencies)
- **SUGGESTION**: Optional improvements (e.g., style enhancements)

## Scoring Guidelines
- 9-10: Excellent, minor polish only
- 7-8.9: Good, ready with minor revisions
- 5-6.9: Acceptable, needs moderate revision
- 3-4.9: Poor, needs major revision
- 0-2.9: Unacceptable, fundamental issues

## Decision Rules
- APPROVE: Score >= 7.5 AND no critical issues AND <= 3 major issues
- REVISE: Score >= 5.0 OR improvable critical/major issues
- REJECT: Score < 5.0 AND issues are not addressable

Today's date: {current_date}
Iteration: {iteration_count} of {max_iterations}
"""

REVIEWER_EVALUATION_TEMPLATE = """## Document to Review

### Metadata
- **Title**: {title}
- **Version**: {version}
- **Sections**: {section_count}
- **Word Count**: {word_count}

### Document Content
{draft_content}

---

### Original Analysis (for fact-checking)
**Key Findings**:
{key_findings}

**Source Citations**:
{citations}

---

### Research Plan (for completeness checking)
**Sub-Questions**:
{sub_questions}

**Expected Sections**:
{expected_sections}

---

## Your Review Task

### Step 1: Criteria Evaluation
For each criterion, assign a score (0-10) and identify specific issues.

### Step 2: Issue Cataloging
List all issues found with:
- Category (accuracy/completeness/coherence/citation/methodology/style)
- Severity (critical/major/minor/suggestion)
- Location (section or general)
- Description
- Suggested fix (if applicable)

### Step 3: Overall Assessment
- Calculate weighted score
- Determine recommendation (approve/revise/reject)
- Provide summary feedback

### Step 4: Improvement Guidance
If recommending revision:
- Prioritize issues to address
- Estimate improvement potential
- Suggest specific fixes

{format_instructions}
"""


async def reviewer_node(state: WorkflowState) -> dict:
    """
    REVIEWER Node Implementation
    
    Evaluates draft quality and generates structured critique.
    
    Args:
        state: Current workflow state with draft
        
    Returns:
        Updated state with critique
    """
    from datetime import datetime
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import PydanticOutputParser
    
    if not state.draft:
        return {
            "errors": ["No draft available for review"],
            "status": ResearchStatus.FAILED
        }
    
    llm = ChatOpenAI(
        model=state.config.get("reviewer_model", "gpt-4o"),
        temperature=0.1  # Low temp for consistent evaluation
    )
    
    parser = PydanticOutputParser(pydantic_object=Critique)
    
    # Format criteria for prompt
    criteria_desc = _format_criteria_description()
    
    # Format draft content
    draft_content = _format_draft_for_review(state.draft)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", REVIEWER_SYSTEM_PROMPT),
        ("human", REVIEWER_EVALUATION_TEMPLATE)
    ])
    
    chain = prompt | llm | parser
    
    try:
        critique = await chain.ainvoke({
            "current_date": datetime.now().strftime("%Y-%m-%d"),
            "iteration_count": state.iteration_count,
            "max_iterations": state.max_iterations,
            "criteria_description": criteria_desc,
            "title": state.draft.title,
            "version": state.draft.version,
            "section_count": len(state.draft.sections),
            "word_count": sum(s.word_count for s in state.draft.sections),
            "draft_content": draft_content,
            "key_findings": "\n".join(state.analysis.key_findings) if state.analysis else "N/A",
            "citations": str(state.analysis.source_citations) if state.analysis else "N/A",
            "sub_questions": "\n".join(state.research_plan.decomposed_questions) if state.research_plan else "N/A",
            "expected_sections": "\n".join(state.research_plan.expected_sections) if state.research_plan else "N/A",
            "format_instructions": parser.get_format_instructions()
        })
        
        # Apply decision rules
        critique.recommendation = _determine_recommendation(critique, state)
        
        # Calculate improvement potential
        improvement_potential = _estimate_improvement_potential(critique)
        
        return {
            "critique": critique,
            "iteration_count": state.iteration_count + 1,
            "status": ResearchStatus.REVIEWING,
            "messages": [AIMessage(content=f"Review complete: Score {critique.overall_score:.1f}/10, Recommendation: {critique.recommendation.upper()}")]
        }
        
    except Exception as e:
        return {
            "errors": [f"Review failed: {str(e)}"],
            "status": ResearchStatus.FAILED
        }


def _format_criteria_description() -> str:
    """Format review criteria for prompt."""
    lines = []
    for name, info in REVIEW_CRITERIA.items():
        lines.append(f"### {name.title()} (Weight: {info['weight']*100:.0f}%)")
        lines.append(f"{info['description']}")
        lines.append("Checks:")
        for check in info['checks']:
            lines.append(f"  - {check}")
        lines.append("")
    return "\n".join(lines)


def _format_draft_for_review(draft: ResearchDraft) -> str:
    """Format draft content for reviewer."""
    parts = []
    
    if draft.abstract:
        parts.append(f"## Abstract\n{draft.abstract}")
    
    for section in draft.sections:
        parts.append(f"## {section.title}\n{section.content}")
    
    if draft.conclusion:
        parts.append(f"## Conclusion\n{draft.conclusion}")
    
    return "\n\n---\n\n".join(parts)


def _determine_recommendation(
    critique: Critique, 
    state: WorkflowState
) -> Literal["approve", "revise", "reject"]:
    """
    Apply decision rules to determine recommendation.
    """
    score = critique.overall_score
    critical_count = critique.critical_count
    major_count = sum(1 for item in critique.items if item.severity == "major")
    
    # Check approval criteria
    if (score >= PASS_THRESHOLD and 
        critical_count <= MAX_CRITICAL_ISSUES and 
        major_count <= MAX_MAJOR_ISSUES):
        return "approve"
    
    # Check if revision is possible
    if score >= REVISION_THRESHOLD or state.can_iterate:
        return "revise"
    
    return "reject"


def _estimate_improvement_potential(critique: Critique) -> float:
    """
    Estimate how much the score could improve with revision.
    
    Based on issue severity and fixability.
    """
    max_improvement = 10.0 - critique.overall_score
    
    # Critical issues are hard to fix
    critical_penalty = critique.critical_count * 0.3
    
    # Major issues are moderately fixable
    major_count = sum(1 for item in critique.items if item.severity == "major")
    major_factor = min(major_count * 0.15, 0.5)
    
    # Minor issues are easily fixable
    minor_count = sum(1 for item in critique.items if item.severity == "minor")
    minor_factor = min(minor_count * 0.05, 0.3)
    
    potential = max_improvement * (1 - critical_penalty) * (0.5 + minor_factor - major_factor)
    
    return max(0.0, min(1.0, potential / max_improvement)) if max_improvement > 0 else 0.0
```

---

**Phase 3 Complete.** ✓

---

## Phase 4: Orchestration, Persistence, & Safety

### 4.1 Edge Logic & Conditional Routing

The graph uses conditional edges to control flow between nodes based on state evaluation.

#### 4.1.1 Edge Definitions

```python
"""
LangGraph Edge Definitions
==========================
Conditional routing logic for workflow orchestration.
"""

from typing import Literal
from langgraph.graph import StateGraph, END, START


# === ROUTING FUNCTIONS ===

def route_after_planner(state: WorkflowState) -> Literal["searcher", "human_review", "end"]:
    """
    Route after Planner node.
    
    Conditions:
    - If HITL enabled for planning: route to human review
    - If plan generation failed: route to end
    - Otherwise: route to searcher
    """
    # Check for errors
    if state.status == ResearchStatus.FAILED:
        return "end"
    
    # Check for HITL breakpoint
    if state.config.get("hitl_after_planning", False):
        return "human_review"
    
    # Check if plan is valid
    if not state.research_plan or not state.research_plan.search_queries:
        state.add_error("Invalid research plan generated")
        return "end"
    
    return "searcher"


def route_after_searcher(state: WorkflowState) -> Literal["searcher", "analyst", "end"]:
    """
    Route after Searcher node.
    
    Conditions:
    - If pending searches remain: continue searching
    - If all searches complete: route to analyst
    - If all searches failed: route to end
    """
    if state.status == ResearchStatus.FAILED:
        return "end"
    
    # Check for pending searches
    if state.pending_searches:
        # Limit search iterations to prevent infinite loops
        search_iterations = state.config.get("_search_iterations", 0)
        if search_iterations >= state.config.get("max_search_iterations", 5):
            state.add_checkpoint("Max search iterations reached, proceeding with available data")
            return "analyst"
        
        state.config["_search_iterations"] = search_iterations + 1
        return "searcher"
    
    # Check if we have any results
    if not state.search_results:
        state.add_error("No search results collected")
        return "end"
    
    return "analyst"


def route_after_analyst(state: WorkflowState) -> Literal["searcher", "writer", "end"]:
    """
    Route after Analyst node.
    
    Conditions:
    - If major gaps and searches allowed: route back to searcher
    - If analysis complete: route to writer
    - If analysis failed: route to end
    """
    if state.status == ResearchStatus.FAILED:
        return "end"
    
    if not state.analysis:
        state.add_error("Analysis generation failed")
        return "end"
    
    # Check for data gaps
    gap_count = len(state.analysis.gaps)
    gap_search_count = state.config.get("_gap_search_count", 0)
    max_gap_searches = state.config.get("max_gap_searches", 2)
    
    if gap_count > 2 and gap_search_count < max_gap_searches:
        # Generate additional searches for gaps
        gap_queries = _generate_gap_searches(state.analysis.gaps[:3])
        if state.research_plan:
            state.research_plan.search_queries.extend(gap_queries)
        state.config["_gap_search_count"] = gap_search_count + 1
        state.add_checkpoint(f"Routing back to search for {gap_count} identified gaps")
        return "searcher"
    
    return "writer"


def route_after_reviewer(
    state: WorkflowState
) -> Literal["writer", "human_review", "output", "fallback"]:
    """
    Route after Reviewer node - the core critique loop logic.
    
    Decision Matrix:
    ┌─────────────────┬──────────────┬─────────────────┬────────────┐
    │ Recommendation  │ Iterations   │ HITL Enabled    │ Route      │
    ├─────────────────┼──────────────┼─────────────────┼────────────┤
    │ approve         │ any          │ yes             │ human      │
    │ approve         │ any          │ no              │ output     │
    │ revise          │ < max        │ any             │ writer     │
    │ revise          │ >= max       │ any             │ fallback   │
    │ reject          │ any          │ any             │ fallback   │
    └─────────────────┴──────────────┴─────────────────┴────────────┘
    """
    if not state.critique:
        state.add_error("No critique generated")
        return "fallback"
    
    recommendation = state.critique.recommendation
    can_iterate = state.iteration_count < state.max_iterations
    hitl_enabled = state.config.get("hitl_before_output", False)
    
    if recommendation == "approve":
        if hitl_enabled:
            return "human_review"
        return "output"
    
    if recommendation == "revise":
        if can_iterate:
            state.add_checkpoint(f"Revision loop iteration {state.iteration_count + 1}")
            return "writer"
        else:
            state.add_checkpoint(f"Max iterations ({state.max_iterations}) reached, returning best effort")
            return "fallback"
    
    # recommendation == "reject"
    state.add_checkpoint("Draft rejected by reviewer")
    return "fallback"


def should_continue_search(state: WorkflowState) -> bool:
    """
    Determine if search should continue.
    
    Loop-breaking conditions:
    1. All queries completed
    2. Max iterations reached
    3. Timeout exceeded
    4. Error threshold exceeded
    """
    # Check completion
    if not state.pending_searches:
        return False
    
    # Check iteration limit
    iterations = state.config.get("_search_iterations", 0)
    if iterations >= state.config.get("max_search_iterations", 5):
        return False
    
    # Check error threshold
    error_count = len([e for e in state.errors if "search" in e.lower()])
    if error_count >= state.config.get("max_search_errors", 3):
        return False
    
    return True
```

#### 4.1.2 Graph Construction

```python
"""
LangGraph Graph Construction
============================
Complete graph assembly with all nodes and edges.
"""

from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver


def build_research_graph() -> StateGraph:
    """
    Construct the complete research workflow graph.
    
    Returns:
        Compiled StateGraph ready for execution
    """
    # Initialize graph with state schema
    graph = StateGraph(WorkflowState)
    
    # === ADD NODES ===
    graph.add_node("planner", planner_node)
    graph.add_node("searcher", searcher_node)
    graph.add_node("analyst", analyst_node)
    graph.add_node("writer", writer_node)
    graph.add_node("reviewer", reviewer_node)
    graph.add_node("human_review", human_review_node)
    graph.add_node("output", output_node)
    graph.add_node("fallback", fallback_node)
    
    # === ADD EDGES ===
    
    # Entry point
    graph.add_edge(START, "planner")
    
    # After Planner
    graph.add_conditional_edges(
        "planner",
        route_after_planner,
        {
            "searcher": "searcher",
            "human_review": "human_review",
            "end": END
        }
    )
    
    # After Searcher (may loop)
    graph.add_conditional_edges(
        "searcher",
        route_after_searcher,
        {
            "searcher": "searcher",  # Loop for pending searches
            "analyst": "analyst",
            "end": END
        }
    )
    
    # After Analyst (may loop back to search)
    graph.add_conditional_edges(
        "analyst",
        route_after_analyst,
        {
            "searcher": "searcher",  # Gap-filling searches
            "writer": "writer",
            "end": END
        }
    )
    
    # After Writer
    graph.add_edge("writer", "reviewer")
    
    # After Reviewer (critique loop)
    graph.add_conditional_edges(
        "reviewer",
        route_after_reviewer,
        {
            "writer": "writer",      # Revision loop
            "human_review": "human_review",
            "output": "output",
            "fallback": "fallback"
        }
    )
    
    # After Human Review
    graph.add_conditional_edges(
        "human_review",
        lambda s: "output" if s.config.get("human_approved", False) else "writer",
        {
            "output": "output",
            "writer": "writer"
        }
    )
    
    # Terminal nodes
    graph.add_edge("output", END)
    graph.add_edge("fallback", END)
    
    return graph


def compile_graph_with_checkpointing() -> StateGraph:
    """
    Compile graph with memory checkpointing for persistence.
    """
    graph = build_research_graph()
    
    # Add memory saver for thread persistence
    memory = MemorySaver()
    
    return graph.compile(
        checkpointer=memory,
        interrupt_before=["human_review"],  # HITL breakpoints
        interrupt_after=[]
    )
```

---

### 4.2 Human-in-the-Loop (HITL) Breakpoints

Strategic interruption points for human oversight and intervention.

#### 4.2.1 HITL Configuration

```python
"""
HITL Breakpoint Configuration
=============================
"""

from dataclasses import dataclass
from typing import Callable, Optional
from enum import Enum


class HITLAction(str, Enum):
    """Actions available at HITL breakpoints."""
    APPROVE = "approve"
    REJECT = "reject"
    MODIFY = "modify"
    SKIP = "skip"


@dataclass
class HITLBreakpoint:
    """Configuration for a HITL breakpoint."""
    name: str
    location: str  # Node name
    trigger: str   # "before" or "after"
    condition: Optional[Callable[[WorkflowState], bool]] = None
    timeout_seconds: int = 3600  # 1 hour default
    auto_approve_on_timeout: bool = False
    required_fields: list = None  # Fields to display for review
    
    def should_trigger(self, state: WorkflowState) -> bool:
        """Check if breakpoint should activate."""
        if self.condition:
            return self.condition(state)
        return True


# === PREDEFINED BREAKPOINTS ===

HITL_BREAKPOINTS = {
    "plan_review": HITLBreakpoint(
        name="Research Plan Review",
        location="planner",
        trigger="after",
        condition=lambda s: s.config.get("hitl_after_planning", False),
        required_fields=["research_plan.decomposed_questions", "research_plan.search_queries"],
        timeout_seconds=1800
    ),
    
    "pre_output_review": HITLBreakpoint(
        name="Final Output Review",
        location="reviewer",
        trigger="after",
        condition=lambda s: (
            s.config.get("hitl_before_output", False) and 
            s.critique and 
            s.critique.recommendation == "approve"
        ),
        required_fields=["draft.title", "draft.abstract", "draft.sections", "critique.overall_score"],
        timeout_seconds=3600
    ),
    
    "critical_issue_review": HITLBreakpoint(
        name="Critical Issue Escalation",
        location="reviewer",
        trigger="after",
        condition=lambda s: s.critique and s.critique.critical_count > 0,
        required_fields=["critique.items", "draft.title"],
        auto_approve_on_timeout=False,
        timeout_seconds=7200
    )
}
```

#### 4.2.2 Human Review Node

```python
"""
Human Review Node Implementation
================================
Handles HITL interruptions and user feedback.
"""

async def human_review_node(state: WorkflowState) -> dict:
    """
    HUMAN_REVIEW Node
    
    This node is a passthrough that captures human feedback.
    The actual interruption is handled by LangGraph's interrupt_before.
    
    When execution resumes, this node processes the human's decision.
    """
    # Get human decision from config (set by external handler)
    human_action = state.config.get("human_action", HITLAction.SKIP)
    human_feedback = state.config.get("human_feedback", "")
    
    if human_action == HITLAction.APPROVE:
        state.config["human_approved"] = True
        return {
            "messages": [AIMessage(content="Human approved the output.")],
            "checkpoints": state.checkpoints + ["Human review: APPROVED"]
        }
    
    elif human_action == HITLAction.REJECT:
        return {
            "messages": [AIMessage(content=f"Human rejected: {human_feedback}")],
            "checkpoints": state.checkpoints + [f"Human review: REJECTED - {human_feedback}"],
            "status": ResearchStatus.FAILED
        }
    
    elif human_action == HITLAction.MODIFY:
        # Human provided modifications to apply
        modifications = state.config.get("human_modifications", {})
        
        # Apply modifications to relevant state fields
        updates = {"checkpoints": state.checkpoints + ["Human review: MODIFICATIONS APPLIED"]}
        
        if "research_plan" in modifications and state.research_plan:
            # Update research plan with human edits
            for key, value in modifications["research_plan"].items():
                setattr(state.research_plan, key, value)
            updates["research_plan"] = state.research_plan
        
        if "draft" in modifications and state.draft:
            # Update draft with human edits
            for key, value in modifications["draft"].items():
                setattr(state.draft, key, value)
            updates["draft"] = state.draft
        
        return updates
    
    # SKIP - continue without changes
    return {
        "checkpoints": state.checkpoints + ["Human review: SKIPPED"]
    }


# === HITL EXTERNAL HANDLER ===

class HITLHandler:
    """
    External handler for HITL breakpoint management.
    
    This class is used by the orchestration layer to:
    1. Present state to human reviewer
    2. Collect feedback
    3. Resume graph execution
    """
    
    def __init__(self, graph, thread_id: str):
        self.graph = graph
        self.thread_id = thread_id
    
    async def get_pending_review(self) -> Optional[dict]:
        """Get current state awaiting human review."""
        state = await self.graph.aget_state({"configurable": {"thread_id": self.thread_id}})
        
        if state.next and "human_review" in state.next:
            return {
                "thread_id": self.thread_id,
                "state_snapshot": state.values,
                "pending_node": "human_review",
                "breakpoint": self._identify_breakpoint(state.values)
            }
        return None
    
    async def submit_review(
        self,
        action: HITLAction,
        feedback: str = "",
        modifications: dict = None
    ) -> dict:
        """Submit human review decision and resume execution."""
        # Update state with human decision
        update_config = {
            "human_action": action,
            "human_feedback": feedback,
            "human_modifications": modifications or {}
        }
        
        # Resume graph execution
        result = await self.graph.ainvoke(
            None,  # No new input, just resume
            config={
                "configurable": {
                    "thread_id": self.thread_id,
                    **update_config
                }
            }
        )
        
        return result
    
    def _identify_breakpoint(self, state: dict) -> str:
        """Identify which breakpoint triggered the pause."""
        for name, bp in HITL_BREAKPOINTS.items():
            if bp.should_trigger(WorkflowState(**state)):
                return name
        return "unknown"
```

---

### 4.3 Persistence & Thread State

LangGraph checkpointing for durable, resumable workflows.

#### 4.3.1 Persistence Configuration

```python
"""
Persistence Layer Configuration
===============================
Support for multiple checkpoint backends.
"""

from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.checkpoint.postgres import PostgresSaver
from typing import Union
import os


class PersistenceConfig:
    """Configuration for workflow persistence."""
    
    def __init__(
        self,
        backend: str = "memory",
        connection_string: str = None,
        ttl_hours: int = 24
    ):
        self.backend = backend
        self.connection_string = connection_string
        self.ttl_hours = ttl_hours
    
    def get_checkpointer(self) -> Union[MemorySaver, SqliteSaver, PostgresSaver]:
        """Get configured checkpointer instance."""
        
        if self.backend == "memory":
            return MemorySaver()
        
        elif self.backend == "sqlite":
            db_path = self.connection_string or "research_workflows.db"
            return SqliteSaver.from_conn_string(f"sqlite:///{db_path}")
        
        elif self.backend == "postgres":
            conn_str = self.connection_string or os.getenv("DATABASE_URL")
            if not conn_str:
                raise ValueError("PostgreSQL connection string required")
            return PostgresSaver.from_conn_string(conn_str)
        
        else:
            raise ValueError(f"Unknown persistence backend: {self.backend}")


# === THREAD MANAGEMENT ===

class ThreadManager:
    """
    Manage workflow threads for persistence and resumption.
    """
    
    def __init__(self, checkpointer):
        self.checkpointer = checkpointer
    
    async def create_thread(self, initial_state: dict) -> str:
        """Create a new workflow thread."""
        import uuid
        thread_id = str(uuid.uuid4())
        
        # Store initial state
        await self.checkpointer.aput(
            config={"configurable": {"thread_id": thread_id}},
            checkpoint={
                "v": 1,
                "ts": datetime.utcnow().isoformat(),
                "channel_values": initial_state,
                "channel_versions": {},
                "versions_seen": {}
            },
            metadata={"created_at": datetime.utcnow().isoformat()}
        )
        
        return thread_id
    
    async def get_thread_state(self, thread_id: str) -> Optional[dict]:
        """Retrieve current state of a thread."""
        checkpoint = await self.checkpointer.aget(
            config={"configurable": {"thread_id": thread_id}}
        )
        
        if checkpoint:
            return checkpoint.get("channel_values", {})
        return None
    
    async def list_active_threads(self) -> List[dict]:
        """List all active (non-completed) threads."""
        threads = []
        
        async for config, checkpoint, metadata in self.checkpointer.alist({}):
            state = checkpoint.get("channel_values", {})
            status = state.get("status", "unknown")
            
            if status not in [ResearchStatus.COMPLETED, ResearchStatus.FAILED]:
                threads.append({
                    "thread_id": config["configurable"]["thread_id"],
                    "status": status,
                    "created_at": metadata.get("created_at"),
                    "last_checkpoint": checkpoint.get("ts")
                })
        
        return threads
    
    async def resume_thread(self, graph, thread_id: str, input_data: dict = None) -> dict:
        """Resume execution of a paused thread."""
        config = {"configurable": {"thread_id": thread_id}}
        
        return await graph.ainvoke(
            input_data,
            config=config
        )
```

#### 4.3.2 State Serialization

```python
"""
State Serialization Helpers
===========================
Handle complex types for persistence.
"""

import json
from datetime import datetime
from pydantic import BaseModel


class StateSerializer:
    """Custom serializer for WorkflowState persistence."""
    
    @staticmethod
    def serialize(state: WorkflowState) -> str:
        """Serialize state to JSON string."""
        return state.model_dump_json(indent=2)
    
    @staticmethod
    def deserialize(data: str) -> WorkflowState:
        """Deserialize JSON string to state."""
        return WorkflowState.model_validate_json(data)
    
    @staticmethod
    def serialize_for_display(state: WorkflowState) -> dict:
        """
        Serialize state for human-readable display.
        Truncates large fields and formats dates.
        """
        data = state.model_dump()
        
        # Truncate large content fields
        if data.get("search_results"):
            for result in data["search_results"]:
                if result.get("snippet") and len(result["snippet"]) > 200:
                    result["snippet"] = result["snippet"][:200] + "..."
                if result.get("full_content"):
                    result["full_content"] = f"[{len(result['full_content'])} chars]"
        
        if data.get("draft") and data["draft"].get("sections"):
            for section in data["draft"]["sections"]:
                if section.get("content") and len(section["content"]) > 500:
                    section["content"] = section["content"][:500] + "..."
        
        # Format dates
        for key in ["created_at", "updated_at"]:
            if data.get(key):
                data[key] = data[key].strftime("%Y-%m-%d %H:%M:%S UTC")
        
        return data
```

---

### 4.4 Error Handling & Recovery

Comprehensive error handling with graceful degradation.

#### 4.4.1 Error Classification

```python
"""
Error Classification and Handling
=================================
"""

from enum import Enum
from dataclasses import dataclass
from typing import Callable, Optional


class ErrorSeverity(str, Enum):
    """Error severity levels."""
    RECOVERABLE = "recoverable"    # Can retry or skip
    DEGRADED = "degraded"          # Continue with reduced quality
    FATAL = "fatal"                # Must stop workflow


class ErrorCategory(str, Enum):
    """Error category classification."""
    API_ERROR = "api_error"           # External API failures
    RATE_LIMIT = "rate_limit"         # Rate limiting
    TIMEOUT = "timeout"               # Operation timeout
    VALIDATION = "validation"         # Data validation failure
    PARSING = "parsing"               # Response parsing failure
    CONTEXT_OVERFLOW = "context"      # Context window exceeded
    INTERNAL = "internal"             # Internal logic error


@dataclass
class WorkflowError:
    """Structured workflow error."""
    category: ErrorCategory
    severity: ErrorSeverity
    message: str
    node: str
    recoverable: bool
    retry_count: int = 0
    max_retries: int = 3
    fallback_action: Optional[str] = None


# === ERROR HANDLERS ===

ERROR_HANDLERS: dict[ErrorCategory, Callable] = {}


def error_handler(category: ErrorCategory):
    """Decorator to register error handlers."""
    def decorator(func):
        ERROR_HANDLERS[category] = func
        return func
    return decorator


@error_handler(ErrorCategory.RATE_LIMIT)
async def handle_rate_limit(error: WorkflowError, state: WorkflowState) -> dict:
    """Handle rate limiting with exponential backoff."""
    import asyncio
    
    if error.retry_count < error.max_retries:
        wait_time = 2 ** error.retry_count * 10  # 10s, 20s, 40s
        await asyncio.sleep(wait_time)
        
        return {
            "errors": state.errors,  # Don't add to errors, will retry
            "_retry": True,
            "_retry_count": error.retry_count + 1
        }
    
    return {
        "errors": state.errors + [f"Rate limit exceeded after {error.max_retries} retries"],
        "status": ResearchStatus.FAILED
    }


@error_handler(ErrorCategory.CONTEXT_OVERFLOW)
async def handle_context_overflow(error: WorkflowError, state: WorkflowState) -> dict:
    """Handle context window overflow with content reduction."""
    
    # Strategy 1: Reduce search results
    if len(state.search_results) > 10:
        # Keep only top relevance results
        sorted_results = sorted(
            state.search_results, 
            key=lambda r: r.relevance_score, 
            reverse=True
        )
        reduced_results = sorted_results[:10]
        
        return {
            "search_results": reduced_results,
            "checkpoints": state.checkpoints + ["Context overflow: reduced to top 10 results"],
            "_retry": True
        }
    
    # Strategy 2: Truncate content
    truncated_results = []
    for result in state.search_results:
        truncated = SearchResult(
            **result.model_dump(),
            snippet=result.snippet[:500] if result.snippet else "",
            full_content=None  # Remove full content
        )
        truncated_results.append(truncated)
    
    return {
        "search_results": truncated_results,
        "checkpoints": state.checkpoints + ["Context overflow: truncated content"],
        "_retry": True
    }


@error_handler(ErrorCategory.API_ERROR)
async def handle_api_error(error: WorkflowError, state: WorkflowState) -> dict:
    """Handle API errors with fallback strategies."""
    
    if error.node == "searcher":
        # Fallback: skip failed search type, continue with others
        if error.retry_count < error.max_retries:
            return {"_retry": True, "_retry_count": error.retry_count + 1}
        
        # Mark affected queries as failed
        if state.research_plan:
            for query in state.research_plan.search_queries:
                if query.status == "pending":
                    query.status = "failed"
        
        return {
            "checkpoints": state.checkpoints + [f"API error in {error.node}: continuing with available data"],
            "errors": state.errors + [error.message]
        }
    
    return {
        "errors": state.errors + [error.message],
        "status": ResearchStatus.FAILED
    }


@error_handler(ErrorCategory.TIMEOUT)
async def handle_timeout(error: WorkflowError, state: WorkflowState) -> dict:
    """Handle operation timeouts."""
    
    if error.retry_count < 2:  # Only 2 retries for timeouts
        return {"_retry": True, "_retry_count": error.retry_count + 1}
    
    return {
        "errors": state.errors + [f"Timeout in {error.node}: {error.message}"],
        "checkpoints": state.checkpoints + [f"Timeout handled for {error.node}"]
    }
```

#### 4.4.2 Fallback Node

```python
"""
Fallback Node Implementation
============================
Graceful degradation when workflow cannot complete normally.
"""

async def fallback_node(state: WorkflowState) -> dict:
    """
    FALLBACK Node
    
    Handles graceful degradation when:
    - Max iterations reached without approval
    - Critical errors prevent completion
    - Reviewer rejects draft
    
    Produces best-effort output with appropriate caveats.
    """
    
    # Determine fallback reason
    fallback_reason = _determine_fallback_reason(state)
    
    # Build fallback output
    fallback_output = {
        "status": "partial",
        "reason": fallback_reason,
        "confidence": "low",
        "caveats": []
    }
    
    # Include best available content
    if state.draft:
        fallback_output["content"] = {
            "title": state.draft.title,
            "abstract": state.draft.abstract,
            "sections": [s.model_dump() for s in state.draft.sections],
            "version": state.draft.version
        }
        fallback_output["caveats"].append(
            f"Draft version {state.draft.version} did not pass final review"
        )
    
    elif state.analysis:
        # No draft, but have analysis
        fallback_output["content"] = {
            "key_findings": state.analysis.key_findings,
            "themes": state.analysis.themes,
            "type": "analysis_only"
        }
        fallback_output["caveats"].append(
            "Writing phase did not complete; raw analysis provided"
        )
    
    elif state.search_results:
        # Only have search results
        fallback_output["content"] = {
            "search_results": [r.model_dump() for r in state.search_results[:10]],
            "type": "search_results_only"
        }
        fallback_output["caveats"].append(
            "Analysis did not complete; raw search results provided"
        )
    
    else:
        fallback_output["content"] = None
        fallback_output["caveats"].append(
            "Workflow failed before collecting meaningful data"
        )
    
    # Add quality warnings
    if state.critique:
        if state.critique.critical_count > 0:
            fallback_output["caveats"].append(
                f"{state.critique.critical_count} critical issues were identified"
            )
        fallback_output["final_score"] = state.critique.overall_score
    
    # Add error summary
    if state.errors:
        fallback_output["errors"] = state.errors[-5:]  # Last 5 errors
    
    return {
        "collected_data": {**state.collected_data, "fallback_output": fallback_output},
        "status": ResearchStatus.COMPLETED,  # Mark as complete (with caveats)
        "messages": [AIMessage(content=f"Workflow completed with fallback: {fallback_reason}")]
    }


def _determine_fallback_reason(state: WorkflowState) -> str:
    """Determine why fallback was triggered."""
    
    if state.iteration_count >= state.max_iterations:
        return f"Maximum revision iterations ({state.max_iterations}) exceeded"
    
    if state.critique and state.critique.recommendation == "reject":
        return "Draft rejected by reviewer"
    
    if state.status == ResearchStatus.FAILED:
        return "Workflow encountered fatal error"
    
    if state.critique and state.critique.critical_count > 0:
        return f"Unresolved critical issues ({state.critique.critical_count})"
    
    return "Unknown fallback trigger"
```

#### 4.4.3 Output Node

```python
"""
Output Node Implementation
==========================
Final output formatting and validation.
"""

async def output_node(state: WorkflowState) -> dict:
    """
    OUTPUT Node
    
    Formats final approved output with metadata.
    """
    
    if not state.draft:
        return {
            "errors": state.errors + ["No draft available for output"],
            "status": ResearchStatus.FAILED
        }
    
    # Build final output
    final_output = {
        "status": "complete",
        "confidence": "high" if state.critique and state.critique.overall_score >= 8.0 else "medium",
        "content": {
            "title": state.draft.title,
            "abstract": state.draft.abstract,
            "sections": [
                {
                    "id": s.section_id,
                    "title": s.title,
                    "content": s.content,
                    "citations": s.citations,
                    "word_count": s.word_count
                }
                for s in state.draft.sections
            ],
            "conclusion": state.draft.conclusion,
            "references": state.draft.references
        },
        "metadata": {
            "workflow_id": state.workflow_id,
            "created_at": state.created_at.isoformat(),
            "completed_at": datetime.utcnow().isoformat(),
            "iterations": state.iteration_count,
            "final_score": state.critique.overall_score if state.critique else None,
            "sources_used": len(state.search_results),
            "word_count": sum(s.word_count for s in state.draft.sections)
        },
        "provenance": {
            "original_query": state.original_query,
            "sub_questions": state.research_plan.decomposed_questions if state.research_plan else [],
            "search_queries_executed": len([
                q for q in (state.research_plan.search_queries if state.research_plan else [])
                if q.status == "completed"
            ]),
            "checkpoints": state.checkpoints
        }
    }
    
    return {
        "collected_data": {**state.collected_data, "final_output": final_output},
        "status": ResearchStatus.COMPLETED,
        "messages": [AIMessage(content=f"Research complete: '{state.draft.title}' ({final_output['metadata']['word_count']} words)")]
    }
```

---

### 4.5 Complete Graph Visualization

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#4a90d9'}}}%%

stateDiagram-v2
    [*] --> Planner: START
    
    Planner --> Searcher: plan_valid
    Planner --> HumanReview: hitl_planning
    Planner --> [*]: plan_failed
    
    HumanReview --> Searcher: approved
    HumanReview --> Planner: modify
    HumanReview --> [*]: rejected
    
    Searcher --> Searcher: pending_queries
    Searcher --> Analyst: all_complete
    Searcher --> [*]: all_failed
    
    Analyst --> Searcher: gaps_major
    Analyst --> Writer: data_sufficient
    Analyst --> [*]: analysis_failed
    
    Writer --> Reviewer: draft_complete
    
    Reviewer --> Writer: revise (iter < max)
    Reviewer --> HumanReview: approve + hitl
    Reviewer --> Output: approve
    Reviewer --> Fallback: reject OR iter >= max
    
    Output --> [*]: COMPLETE
    Fallback --> [*]: COMPLETE (partial)
    
    note right of Planner: CoT Decomposition
    note right of Searcher: ReAct Pattern
    note right of Reviewer: Critique Loop
    note right of HumanReview: HITL Breakpoint
```

---

**Phase 4 Complete.** ✓

---

## Summary

This specification defines a complete LangGraph-based Multi-Agent Research Workflow with:

| Component | Description |
|-----------|-------------|
| **State Schema** | Pydantic models with LangGraph message annotation |
| **5 Core Nodes** | Planner, Searcher, Analyst, Writer, Reviewer |
| **3 Support Nodes** | Human Review, Output, Fallback |
| **Prompting Strategies** | CoT for Planner, ReAct for Searcher |
| **Tools** | Web, Academic, News search + Content parser |
| **Critique Loop** | 6-criteria weighted scoring with approve/revise/reject |
| **HITL** | Configurable breakpoints with timeout handling |
| **Persistence** | Memory, SQLite, PostgreSQL backends |
| **Error Handling** | Category-based handlers with graceful degradation |

### Quick Start

```python
# Build and compile graph
graph = compile_graph_with_checkpointing()

# Create thread and run
thread_id = "research-001"
result = await graph.ainvoke(
    {
        "original_query": "What are the economic impacts of AI on labor markets?",
        "config": {
            "depth": "comprehensive",
            "hitl_before_output": True,
            "max_iterations": 3
        }
    },
    config={"configurable": {"thread_id": thread_id}}
)

# Access final output
print(result["collected_data"]["final_output"])
```

---

**All Phases Complete.** ✓

---

## Appendix A: Requirements Traceability

This matrix maps the original requirements to their implementation locations in this specification.

### Phase 1 Requirements

| Requirement | Section | Status |
|-------------|---------|--------|
| State Schema (TypedDict/Pydantic) | [1.1](#11-state-schema-definition) | ✅ Complete |
| Fields: `messages` | WorkflowState.messages | ✅ Complete |
| Fields: `research_plan` | WorkflowState.research_plan | ✅ Complete |
| Fields: `collected_data` | WorkflowState.collected_data | ✅ Complete |
| Fields: `critique` | WorkflowState.critique | ✅ Complete |
| Fields: `iteration_count` | WorkflowState.iteration_count | ✅ Complete |
| Mermaid.js Topology | [1.2](#12-high-level-node-topology) | ✅ Complete |
| Node: Planner | [2.1](#21-planner-node) | ✅ Complete |
| Node: Searcher | [2.2](#22-searcher-node) | ✅ Complete |
| Node: Analyst | [3.1](#31-analyst-node) | ✅ Complete |
| Node: Writer | [3.2](#32-writer-node) | ✅ Complete |
| Node: Reviewer | [3.3](#33-reviewer-node) | ✅ Complete |

### Phase 2 Requirements

| Requirement | Section | Status |
|-------------|---------|--------|
| Planner Input/Output Schema | [2.1.1](#211-inputoutput-schema) | ✅ Complete |
| Planner Internal Logic (CoT) | [2.1.2](#212-internal-logic--chain-of-thought-strategy) | ✅ Complete |
| Searcher Input/Output Schema | [2.2.1](#221-inputoutput-schema) | ✅ Complete |
| Searcher Internal Logic (ReAct) | [2.2.3](#223-internal-logic--react-pattern) | ✅ Complete |
| Tool: Web Search API | [2.2.2](#222-tool-definitions--search-apis) | ✅ Complete |
| Tool: Academic Search API | [2.2.2](#222-tool-definitions--search-apis) | ✅ Complete |
| Tool: News Search API | [2.2.2](#222-tool-definitions--search-apis) | ✅ Complete |
| Tool: Content Parser | [2.2.2](#222-tool-definitions--search-apis) | ✅ Complete |

### Phase 3 Requirements

| Requirement | Section | Status |
|-------------|---------|--------|
| Analyst Synthesis Logic | [3.1.2](#312-internal-logic--synthesis-strategy) | ✅ Complete |
| Writer Generation Logic | [3.2.2](#322-internal-logic--structured-generation) | ✅ Complete |
| Reviewer Critique Criteria | [3.3.2](#332-critique-loop-criteria) | ✅ Complete |
| Reviewer Prompt Templates | [3.3.3](#333-internal-logic--structured-critique) | ✅ Complete |
| Critique Loop Decision Rules | [3.3.2](#332-critique-loop-criteria) | ✅ Complete |

### Phase 4 Requirements

| Requirement | Section | Status |
|-------------|---------|--------|
| Edge Logic / Conditional Routing | [4.1.1](#411-edge-definitions) | ✅ Complete |
| `should_continue` Logic | [4.1.1](#411-edge-definitions) | ✅ Complete |
| Loop-Breaking Conditions | [4.1.1](#411-edge-definitions) | ✅ Complete |
| HITL Breakpoints | [4.2](#42-human-in-the-loop-hitl-breakpoints) | ✅ Complete |
| Thread State Saving | [4.3](#43-persistence--thread-state) | ✅ Complete |
| Context Window Fallback | [4.4.1](#441-error-classification) | ✅ Complete |
| Error Handling | [4.4](#44-error-handling--recovery) | ✅ Complete |

---

## Appendix B: Configuration Reference

### WorkflowState Configuration Options

| Config Key | Type | Default | Description |
|------------|------|---------|-------------|
| `depth` | string | `"standard"` | Research depth: `quick`, `standard`, `comprehensive` |
| `style_guide` | string | `"academic"` | Writing style: `academic`, `technical`, `executive` |
| `target_length` | int | `2000` | Target word count for output |
| `max_iterations` | int | `3` | Maximum revision iterations |
| `max_search_iterations` | int | `5` | Maximum search loop iterations |
| `max_gap_searches` | int | `2` | Maximum gap-filling search rounds |
| `max_search_errors` | int | `3` | Error threshold before stopping search |
| `hitl_after_planning` | bool | `False` | Enable HITL after planning phase |
| `hitl_before_output` | bool | `False` | Enable HITL before final output |
| `planner_model` | string | `"gpt-4o"` | LLM model for Planner node |
| `searcher_model` | string | `"gpt-4o-mini"` | LLM model for Searcher node |
| `analyst_model` | string | `"gpt-4o"` | LLM model for Analyst node |
| `writer_model` | string | `"gpt-4o"` | LLM model for Writer node |
| `reviewer_model` | string | `"gpt-4o"` | LLM model for Reviewer node |

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | Yes | OpenAI API key for LLM access |
| `SERPER_API_KEY` | For web search | Serper API key for web search |
| `NEWS_API_KEY` | For news search | NewsAPI key for news search |
| `DATABASE_URL` | For PostgreSQL | PostgreSQL connection string |

### Persistence Backend Selection

```python
# Memory (default, non-persistent)
config = PersistenceConfig(backend="memory")

# SQLite (local persistence)
config = PersistenceConfig(
    backend="sqlite",
    connection_string="research_workflows.db"
)

# PostgreSQL (production persistence)
config = PersistenceConfig(
    backend="postgres",
    connection_string="postgresql://user:pass@host:5432/db"
)
```

### HITL Timeout Configuration

```python
# Custom HITL breakpoint with extended timeout
custom_breakpoint = HITLBreakpoint(
    name="Extended Review",
    location="reviewer",
    trigger="after",
    condition=lambda s: s.critique.overall_score < 6.0,
    timeout_seconds=14400,  # 4 hours
    auto_approve_on_timeout=False
)
```

---

*Document Version: 1.1 | Author: Gia Tenica* | Date: 2 January 2026*
